# -*- coding: utf-8 -*-
"""Copy of chatbot_BOW_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-mkpqfl3XhGD5IGeubxKOlvIyUQrq7ft
"""

import json
import string
import random
import nltk
import numpy as np
from nltk.stem import WordNetLemmatizer
import tensorflow as tensorF
import dill as pickle
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout

nltk.download("punkt")
nltk.download("wordnet")

nltk.download('omw-1.4')

data = {"intents":
        [
            {
                "tag": "greetings",
                "patterns": ["hello", "hey", "hi", "good day", "greetings", "what's up?", "how is it going"],
                "responses":["hello", "hey!", "what can i do for you?"]
            },
            {
                "tag": "goodbye",
                "patterns": ["cya", "see you later", "goodbye", "have a good day", "bye", "cao", "see ya"],
                "responses":["have a nice day", "goodbye"]
            },
            {
                "tag": "age",
                "patterns": ["how old", "how old are you?", "what is your age", "how old are you", "age?"],
                "responses":["I get reborn after every compilation", "hey!", "my owners are averagely 20 years!"]
            },
            {
                "tag": "name",
                "patterns": ["what is your name", "what should i call you", "what's your name?", "who are you?", "can you tell me your name"],
                "responses":["you can call me Medbot!", "i am Medbot!", "i am Medbot your medical assistant"]
            },
            {
                "tag": "error in registration",
                "patterns": ["Error in registration",
                             "can't register"],
                "responses":["Please go throgh the guidelines-https://enam.gov.in/web/resources/registration-guideline and if the problem still exists,please rasie a token for the same"]
            },
            {
                "tag": "Account associated with mobile number not found",
                "patterns": ["Account associated with mobile number not found"],
                "responses":["Please try again after 1 hour. If problem still exists then raise a token for the same"]
            },
            {
                "tag": "OTP not received",
                "patterns": ["OTP not received"],
                "responses":["Plese check your details again.If problem stil exists then retry after some time"]
            },
            {
                "tag": "Token raised but problem still not resolved",
                "patterns": ["Token raised but problems still not resolved"],
                "responses":["Sorry for the inconvenience. I have assigned you assigned priority. Your problem will be resolved ASAP"]
            }
]}

lm = WordNetLemmatizer()
ourClasses = []
newWords = []
docPattern = []
docTag = []

for intent in data['intents']:
    for pattern in intent['patterns']:
        ourNewTokens = nltk.word_tokenize(pattern)
        newWords.extend(ourNewTokens)
        docPattern.append(pattern)
        docTag.append(intent['tag'])
    if intent['tag'] not in ourClasses:
        ourClasses.append(intent['tag'])
# print(newWords)
newWords = [lm.lemmatize(word.lower())
            for word in newWords if word not in string.punctuation]
# print(newWords)
newWords = sorted(set(newWords))
# print(ourClasses)
ourClasses = sorted(set(ourClasses))

print(len(ourClasses))

print(newWords)

print(docPattern)

print(docTag)

trainingData = []
outEmpty = [0]*len(ourClasses)

for i, doc in enumerate(docPattern):
    bagOfWords = []
    text = lm.lemmatize(doc.lower())
    # print(text)
    for word in newWords:
        bagOfWords.append(1) if word in text else bagOfWords.append(0)
    outputRow = list(outEmpty)
    # print(bagOfWords)

    outputRow[ourClasses.index(docTag[i])] = 1
    # print(outputRow)
    trainingData.append([bagOfWords, outputRow])

random.shuffle(trainingData)
trainingData = np.array(trainingData, dtype=object)

x = np.array(list(trainingData[:, 0]))
y = np.array(list(trainingData[:, 1]))

print(x)

print(y)

iShape = (len(x[0]),)
oShape = len(y[0])

ourNewModel = Sequential()
ourNewModel.add(Dense(128, input_shape=iShape, activation='relu'))
ourNewModel.add(Dropout(0.5))
ourNewModel.add(Dense(64, activation="relu"))
ourNewModel.add(Dropout(0.3))
ourNewModel.add(Dense(oShape, activation='softmax'))
md = tensorF.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)
ourNewModel.compile(loss='categorical_crossentropy',
                    optimizer=md,
                    metrics=['accuracy'])
print(ourNewModel.summary())
ourNewModel.fit(x, y, epochs=200, verbose=1)


def ourText(text):
    newTokens = nltk.word_tokenize(text)
    newTokens = [lm.lemmatize(word) for word in newTokens]
    return newTokens


def wordBag(text, vocab):
    newTokens = ourText(text)
    bagOfWords = [0]*len(vocab)
    for w in newTokens:
        for i, word in enumerate(vocab):
            if word == w:
                bagOfWords[i] = 1
    return np.array(bagOfWords)


def PClass(text, vocab, labels):
    bagOfWords = wordBag(text, vocab)
    ourResult = ourNewModel.predict(np.array([bagOfWords]))[0]
    newThresh = 0.2
    yp = [[i, res]for i, res in enumerate(ourResult) if res > newThresh]
    print(ourResult)
    print(yp)
    yp.sort(key=lambda x: x[1], reverse=True)
    newList = []
    for r in yp:
        newList.append(labels[r[0]])
    return newList


def getRes(firstList, fJson):
    tag = firstList[0]
    listOfIntents = fJson['intents']
    for i in listOfIntents:
        if i['tag'] == tag:
            ourResult = random.choice(i['responses'])
            break
    return ourResult


# while True:
#     newMessage = input("")
#     intents = PClass(newMessage, newWords, ourClasses)
#     ourResult = getRes(intents, data)
#     print(ourResult)


print("Pickling starts")


with open('PClass.pkl', 'wb') as file:
  pickle.dump(PClass,file)

with open('getRes.pkl', 'wb') as file:
  pickle.dump(getRes,file)

with open('newWords.pkl', 'wb') as file:
  pickle.dump(newWords,file)


with open('ourClasses.pkl', 'wb') as file:
  pickle.dump(ourClasses,file)

with open('data.pkl', 'wb') as file:
  pickle.dump(data,file)

with open('wordBag.pkl', 'wb') as file:
  pickle.dump(wordBag,file)

with open('ourText.pkl', 'wb') as file:
  pickle.dump(ourText,file)

ourNewModel.save('ourNewModel.h5')

getRes_pkl=pickle.dumps(getRes)
newWords_pkl=pickle.dumps(newWords) 
ourClasses_pkl=pickle.dumps(ourClasses) 
data_pkl=pickle.dumps(data) 
print("Pickling ends")